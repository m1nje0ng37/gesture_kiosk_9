{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb329b7",
   "metadata": {},
   "source": [
    "# 새로운 제스처 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b788fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good (826, 100)\n",
      "good (796, 30, 100)\n",
      "bad (885, 100)\n",
      "bad (855, 30, 100)\n",
      "four (881, 100)\n",
      "four (851, 30, 100)\n",
      "five (889, 100)\n",
      "five (859, 30, 100)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time, os\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 인식할 동작 목록 정의\n",
    "actions = ['good', 'bad', 'four', 'five']\n",
    "\n",
    "# 각 시퀀스의 길이와 각 동작의 지속 시간 정의\n",
    "seq_length = 30\n",
    "secs_for_action = 30\n",
    "\n",
    "# MediaPipe hands 모델 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 데이터셋을 저장할 디렉토리 생성\n",
    "created_time = int(time.time())\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "\n",
    "# 각 동작에 대한 데이터를 수집하기 위한 루프\n",
    "while cap.isOpened():\n",
    "    for idx, action in enumerate(actions):\n",
    "        data = [] # 현재 동작에 대한 데이터를 저장할 리스트\n",
    "        \n",
    "        # 웹캠에서 프레임을 읽어옴\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        img = cv2.flip(img, 1) # 이미지를 좌우로 뒤집어 거울 효과\n",
    "\n",
    "        # 수집할 동작을 표시하는 메시지 출력\n",
    "        cv2.putText(img, f'Waiting for collecting {action.upper()} action...', \n",
    "                    org=(10, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.waitKey(3000) # 데이터 수집 시작 전에 3초 대기\n",
    "\n",
    "        start_time = time.time() # 시작 시간 기록\n",
    "        \n",
    "        # 지정된 기간 동안 데이터 수집\n",
    "        while time.time() - start_time < secs_for_action:\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            img = cv2.flip(img, 1) # 이미지를 좌우로 뒤집음\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 이미지를 RGB로 변환\n",
    "            result = hands.process(img) # 이미지를 처리하여 손 검출\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # 이미지를 BRG로 다시 변환\n",
    "        \n",
    "            # 이미지에서 손이 검출된 경우\n",
    "            if result.multi_hand_landmarks is not None:\n",
    "                for res in result.multi_hand_landmarks:\n",
    "                    joint = np.zeros((21, 4)) # 관절 좌표를 저장할 배열 초기화\n",
    "                    for j, lm in enumerate(res.landmark):\n",
    "                        joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "                    # 관절 간의 각도 계산\n",
    "                    v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "                    v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "                    v = v2 - v1 # [20, 3] # 관절 간의 벡터\n",
    "                    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis] # 벡터 정규화\n",
    "\n",
    "                    # 내적의 아크 코사인을 사용하여 각도 계산\n",
    "                    angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                        v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                        v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "                                        \n",
    "                    angle = np.degrees(angle) # 라디안을 각도로 변환\n",
    "                    \n",
    "                    # 각도와 동작 인덱스를 데이터에 추가\n",
    "                    angle_label = np.array([angle], dtype=np.float32)\n",
    "                    angle_label = np.append(angle_label, idx)\n",
    "\n",
    "                    d = np.concatenate([joint.flatten(), angle_label])\n",
    "\n",
    "                    data.append(d)\n",
    "                    \n",
    "                    # 이미지에 손 랜드마크 그리기\n",
    "                    mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # 손 랜드마크가 그려진 이미지 표시\n",
    "            cv2.imshow('img', img)\n",
    "            if cv2.waitKey(1) == ord('q'): # 'q' 키를 누르면 종료\n",
    "                break\n",
    "\n",
    "        # 원시 데이터 저장\n",
    "        data = np.array(data)\n",
    "        print(action, data.shape)\n",
    "        np.save(os.path.join('dataset', f'raw_{action}_{created_time}'), data)\n",
    "\n",
    "        # 원시 데이터에서 시퀀스 데이터 생성\n",
    "        full_seq_data = []\n",
    "        for seq in range(len(data) - seq_length):\n",
    "            full_seq_data.append(data[seq:seq + seq_length])\n",
    "\n",
    "        full_seq_data = np.array(full_seq_data)\n",
    "        print(action, full_seq_data.shape)\n",
    "        np.save(os.path.join('dataset', f'seq_{action}_{created_time}'), full_seq_data)\n",
    "    break # 모든 동작에 대한 데이터 수집 후 while 루프 종료\n",
    "    \n",
    "# 웹캠 해제 및 모든 OpenCV 창 닫기\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 기존 K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad948ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['good', 'bad', 'four', 'five']\n",
    "# 위의 기존 코드와 동일하게 데이터를 수집합니다.\n",
    "# 수집한 데이터를 'dataset' 디렉토리에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78404d7a",
   "metadata": {},
   "source": [
    "# 기존 데이터와 새로운 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc57217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 생성 시간 변수 설정 (예: 임의의 시간)\n",
    "created_time = 1716892952\n",
    "\n",
    "# 기존 데이터 로드\n",
    "existing_actions = ['one', 'two', 'three']\n",
    "existing_data = []\n",
    "for action in existing_actions:\n",
    "    seq_data = np.load(f'dataset/seq_{action}_{created_time}.npy')\n",
    "    existing_data.append(seq_data)\n",
    "\n",
    "existing_data = np.concatenate(existing_data, axis=0)\n",
    "\n",
    "# 생성 시간 변수 설정 (예: 임의의 시간)\n",
    "created_time = 1718526787\n",
    "\n",
    "# 새로운 데이터 로드\n",
    "new_actions = ['good', 'bad', 'four', 'five']\n",
    "new_data = []\n",
    "for action in new_actions:\n",
    "    seq_data = np.load(f'dataset/seq_{action}_{created_time}.npy')\n",
    "    new_data.append(seq_data)\n",
    "\n",
    "new_data = np.concatenate(new_data, axis=0)\n",
    "\n",
    "# 기존 데이터와 새로운 데이터 병합\n",
    "total_data = np.concatenate([existing_data, new_data], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a4fcd",
   "metadata": {},
   "source": [
    "# 모델 확장 및 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d36da78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 7), output.shape=(None, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 모델 재학습\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:553\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[1;32m--> 553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    556\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    557\u001b[0m         )\n\u001b[0;32m    559\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[0;32m    560\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 7), output.shape=(None, 3)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 기존 모델 로드\n",
    "model = load_model('models/model.keras')\n",
    "\n",
    "# 입력 데이터와 레이블 준비\n",
    "X = total_data[:, :, :-1]\n",
    "y = total_data[:, 0, -1]\n",
    "\n",
    "# 원-핫 인코딩\n",
    "y = to_categorical(y, num_classes=len(existing_actions) + len(new_actions))\n",
    "\n",
    "# 모델 재학습\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# 모델 저장\n",
    "model.save('models/model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7fe71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
