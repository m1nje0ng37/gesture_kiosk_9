{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff87c25a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=models/model.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 미리 훈련된 모델을 불러옴\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# MediaPipe hands 모델 초기화\u001b[39;00m\n\u001b[1;32m     15\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_api.py:187\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    184\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: filepath=models/model.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# 인식할 손동작 클래스들을 정의\n",
    "actions = ['one', 'two', 'three']\n",
    "seq_length = 30\n",
    "\n",
    "# 미리 훈련된 모델을 불러옴\n",
    "model = load_model('models/model.keras')\n",
    "\n",
    "# MediaPipe hands 모델 초기화\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "\n",
    "# 웹캠을 사용하여 비디오 캡처를 시작함\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "# out = cv2.VideoWriter('input.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (w, h))\n",
    "# out2 = cv2.VideoWriter('output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (w, h))\n",
    "\n",
    "seq = [] # 손동작 시퀀스를 저장할 리스트\n",
    "action_seq = [] # 인식된 손동작 시퀀스를 저장할 리스트\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    img0 = img.copy()\n",
    "\n",
    "    img = cv2.flip(img, 1) # 이미지를 좌우 반전\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV의 BGR 이미지를 RGB 이미지로 변환\n",
    "    result = hands.process(img) # MediaPipe Hands 모델을 사용하여 손을 감지\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # RGB 이미지를 다시 BGR 이미지로 변환\n",
    "    \n",
    "    # 손을 감지한 경우\n",
    "    if result.multi_hand_landmarks is not None:\n",
    "        for res in result.multi_hand_landmarks:\n",
    "            joint = np.zeros((21, 4))\n",
    "            for j, lm in enumerate(res.landmark):\n",
    "                joint[j] = [lm.x, lm.y, lm.z, lm.visibility]\n",
    "\n",
    "            # 관절 간의 각도를 계산\n",
    "            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3] # Parent joint\n",
    "            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3] # Child joint\n",
    "            v = v2 - v1 # [20, 3]\n",
    "            # Normalize v\n",
    "            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "            # Get angle using arcos of dot product\n",
    "            angle = np.arccos(np.einsum('nt,nt->n',\n",
    "                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], \n",
    "                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:])) # [15,]\n",
    "\n",
    "            angle = np.degrees(angle) # Convert radian to degree\n",
    "\n",
    "            d = np.concatenate([joint.flatten(), angle])\n",
    "\n",
    "            seq.append(d)\n",
    "\n",
    "            mp_drawing.draw_landmarks(img, res, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 손동작 시퀀스의 길이가 지정된 길이보다 작은 경우\n",
    "            if len(seq) < seq_length:\n",
    "                continue\n",
    "\n",
    "            # 입력 데이터를 준비\n",
    "            input_data = np.expand_dims(np.array(seq[-seq_length:], dtype=np.float32), axis=0)\n",
    "\n",
    "            # 모델을 사용하여 손동작을 예측\n",
    "            y_pred = model.predict(input_data).squeeze()\n",
    "\n",
    "            # 가장 높은 확률을 가진 클래스를 찾음\n",
    "            i_pred = int(np.argmax(y_pred))\n",
    "            conf = y_pred[i_pred]\n",
    "\n",
    "            # 예측 확률이 일정 임계값 이상인 경우에만 인식\n",
    "            if conf < 0.9:\n",
    "                continue\n",
    "\n",
    "            # 예측된 손동작을 저장\n",
    "            action = actions[i_pred]\n",
    "            action_seq.append(action)\n",
    "\n",
    "            # 인식된 손동작 시퀀스의 길이가 3 미만인 경우에는 무시\n",
    "            if len(action_seq) < 3:\n",
    "                continue\n",
    "\n",
    "            this_action = '?' # 기본값으로 물음표를 설정\n",
    "            if action_seq[-1] == action_seq[-2] == action_seq[-3]:\n",
    "                this_action = action\n",
    "\n",
    "            # 해당 동작을 이미지에 텍스트로 표시\n",
    "            cv2.putText(img, f'{this_action.upper()}', org=(int(res.landmark[0].x * img.shape[1]), int(res.landmark[0].y * img.shape[0] + 20)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "    # out.write(img0)\n",
    "    # out2.write(img)\n",
    "    # 이미지를 화면에 표시\n",
    "    cv2.imshow('img', img)\n",
    "    # 키 입력을 대기하며 'q' 키를 누르면 프로그램을 종료\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8d97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc72952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹캠 정상 작동 테스트\n",
    "# 실행해서 웹캠 잘 나오면 cv 웹캠은 이상 없는 것\n",
    "\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Camera Test', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
